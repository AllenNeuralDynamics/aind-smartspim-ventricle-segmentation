{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae6e835-9624-40c3-b8ee-d8e682534ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "import os\n",
    "from patchify import patchify  #Only to handle large images\n",
    "import random\n",
    "from scipy import ndimage\n",
    "from pathlib import Path\n",
    "import zarr\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "BASE_PATH = Path(\"/scratch/ventricle_dataset/train\")\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
    "    if interArea == 0:\n",
    "        return 0\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "    # print(boxAArea, boxBArea)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "\n",
    "def filter_bboxes_by_area(bboxes, iou_threshold=0.5):\n",
    "    # Calculate the area of each bounding box\n",
    "    areas = [(box[2] - box[0]) * (box[3] - box[1]) for box in bboxes]\n",
    "    \n",
    "    # Sort bounding boxes by area in descending order\n",
    "    sorted_indices = np.argsort(-np.array(areas))\n",
    "    bboxes = [bboxes[i] for i in sorted_indices]\n",
    "    # print(\"Sorted boxes: \", bboxes)\n",
    "    selected_bboxes = []\n",
    "\n",
    "    while bboxes:\n",
    "        # Choose the bounding box with the largest area\n",
    "        chosen_box = bboxes.pop(0)\n",
    "        selected_bboxes.append(chosen_box)\n",
    "\n",
    "        # for box in bboxes:\n",
    "        #     iou = compute_iou(chosen_box, box)\n",
    "        #     print(f\"IOU between {chosen_box} and {box}: {iou}\")\n",
    "        #     if iou > iou_threshold:\n",
    "        #         print(\"Removing \", box)\n",
    "\n",
    "            # else:\n",
    "                \n",
    "        # Remove boxes that overlap with the chosen box\n",
    "        bboxes = [box for box in bboxes if compute_iou(chosen_box, box) < iou_threshold]\n",
    "\n",
    "    return selected_bboxes\n",
    "\n",
    "def get_bounding_box(ground_truth_map, iou_threshold=0.5):\n",
    "    ground_truth_map = np.squeeze(ground_truth_map)\n",
    "    H, W = ground_truth_map.shape\n",
    "    labeled_mask = label(ground_truth_map)\n",
    "    regions = regionprops(labeled_mask)\n",
    "    bboxes = []\n",
    "    \n",
    "    for r in regions:\n",
    "        min_row, min_col, max_row, max_col = r.bbox\n",
    "        min_col = max(0, min_col - np.random.randint(0, 20))\n",
    "        max_col = min(W, max_col + np.random.randint(0, 20))\n",
    "        min_row = max(0, min_row - np.random.randint(0, 20))\n",
    "        max_row = min(H, max_row + np.random.randint(0, 20))\n",
    "        \n",
    "        bboxes.append([\n",
    "            float(min_col),\n",
    "            float(min_row),\n",
    "            float(max_col),\n",
    "            float(max_row),\n",
    "        ])\n",
    "        bboxes = filter_bboxes_by_area(bboxes, iou_threshold=iou_threshold)\n",
    "        bboxes = [list(b) for b in bboxes]\n",
    "        # print(\"Filtered bboxs: \", bboxes)\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, image_path, labels_path, points, processor, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list or numpy array): List of images as NumPy arrays.\n",
    "            labels (list or numpy array): List of labels as NumPy arrays or other formats.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.labels_paths = natsorted(\n",
    "            [str(p.name) for p in list(Path(image_path).glob(\"*.npy\"))]\n",
    "        )\n",
    "        self.image_base_path = Path(image_path)\n",
    "        self.labels_base_path = Path(labels_path)\n",
    "        self.points = points.copy()\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.labels_paths[idx]\n",
    "        slice_idx = int(path.split('_')[-1].replace('.npy', ''))\n",
    "        slice_points = self.points[self.points[:, 0] == slice_idx]\n",
    "        if not slice_points.shape[0]:\n",
    "            raise ValueError(f\"Problem getting points from image {path} {slice_idx}\")\n",
    "\n",
    "        # slice_points = slice_points[:, 1:]\n",
    "        image = np.load(self.image_base_path.joinpath(self.labels_paths[idx])).astype(np.float32)\n",
    "        label = np.load(self.labels_base_path.joinpath(self.labels_paths[idx])).astype(np.uint8)\n",
    "        bbox_prompt = get_bounding_box(label)\n",
    "        \n",
    "        if self.transform:\n",
    "            bbox_prompt = [ b + ['ventricle'] for b in bbox_prompt]\n",
    "            augmented_data = self.transform(\n",
    "                image=image,\n",
    "                mask=label,\n",
    "                bboxes=bbox_prompt,\n",
    "            )\n",
    "            image = augmented_data['image'].detach().cpu().numpy()\n",
    "            label = augmented_data['mask'].detach().cpu().numpy()\n",
    "            bbox_prompt = augmented_data['bboxes']\n",
    "            bbox_prompt = [list(b[:-1]) for b in bbox_prompt]\n",
    "\n",
    "        else:\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "        # if not isinstance(image, torch.Tensor):\n",
    "        #     image = torch.tensor(image[np.newaxis, ...], dtype=torch.float32)\n",
    "\n",
    "        # if not isinstance(label, torch.Tensor):\n",
    "        #     label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        image = image.transpose( (1,2,0) ) / 255\n",
    "        image = np.concatenate([image]*3, axis=-1)\n",
    "        # print(bbox_prompt)\n",
    "\n",
    "        # print(image.shape, label.shape,image.dtype, label.dtype, len(slice_points), slice_points)\n",
    "        # slice_points.tolist()\n",
    "        inputs = self.processor(image, input_boxes=[bbox_prompt], return_tensors=\"pt\", input_data_format=\"channels_last\", do_rescale=False)\n",
    "    \n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "    \n",
    "        # add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = label\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d5d328-0fbc-46c0-be97-91ae30efae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # print(batch[0].keys())\n",
    "    pixel_values = []\n",
    "    original_sizes = []\n",
    "    reshaped_input_sizes = []\n",
    "    input_boxes = []\n",
    "    ground_truth_mask = []\n",
    "\n",
    "    for b in batch:\n",
    "        pixel_values.append(b['pixel_values'])\n",
    "        original_sizes.append(b['original_sizes'])\n",
    "        reshaped_input_sizes.append(b['reshaped_input_sizes'])\n",
    "        input_boxes.append(b['input_boxes'])\n",
    "        ground_truth_mask.append(b['ground_truth_mask'])\n",
    "\n",
    "    return pixel_values, original_sizes, reshaped_input_sizes, input_boxes, ground_truth_mask\n",
    "\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "BASE_PATH = Path(\"/scratch/ventricle_dataset/train\")\n",
    "\n",
    "points = np.load(BASE_PATH.joinpath('points/smartspim_693196_vs_pts.npy'))\n",
    "ventricle_dataset = NumpyDataset(\n",
    "    image_path=BASE_PATH.joinpath('images'),\n",
    "    labels_path=BASE_PATH.joinpath('labels'),\n",
    "    points=points,\n",
    "    processor=processor,\n",
    "    transform=None, #augmentations\n",
    ")\n",
    "dataloader = DataLoader(ventricle_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62d12ab-5d4a-4f21-9bf9-bd53a0640f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = torch.device(0)\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")#, low_cpu_mem_usage=True, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2c7b44-9861-47fe-a14d-e10a3df0125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3751/2804544273.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ventricle_weights = torch.load(\"sam_ventricle_2_0.7978167533874512.pt\")#\"only_decoder/sam_ventricle_40_0.7864770293235779.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SamModel(\n",
       "  (shared_image_embedding): SamPositionalEmbedding()\n",
       "  (vision_encoder): SamVisionEncoder(\n",
       "    (patch_embed): SamPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x SamVisionLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): SamVisionAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SamMLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): SamVisionNeck(\n",
       "      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): SamPromptEncoder(\n",
       "    (shared_embedding): SamPositionalEmbedding()\n",
       "    (mask_embed): SamMaskEmbedding(\n",
       "      (activation): GELUActivation()\n",
       "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "    (point_embed): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): SamMaskDecoder(\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (transformer): SamTwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
       "          (self_attn): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SamMLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): SamAttention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_layer_norm): SamLayerNorm()\n",
       "    (activation): GELU(approximate='none')\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x SamFeedForward(\n",
       "        (activation): ReLU()\n",
       "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): SamFeedForward(\n",
       "      (activation): ReLU()\n",
       "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ventricle_weights = torch.load(\"sam_ventricle_2_0.7978167533874512.pt\")#\"only_decoder/sam_ventricle_40_0.7864770293235779.pt\")\n",
    "model.load_state_dict(ventricle_weights)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a583ae54-347d-4435-943d-1a9c97152f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [03:20<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(dataloader)\n",
    "\n",
    "volume_slices = []\n",
    "volume_masks = []\n",
    "volume_pred = []\n",
    "volume_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in pbar:\n",
    "        # Getting data from batch\n",
    "        pixel_values, original_sizes, reshaped_input_sizes, input_boxes, ground_truth_masks = batch\n",
    "    \n",
    "        pixel_values = torch.stack(pixel_values, dim=0).to(device)#, torch.float16)\n",
    "        ground_truth_masks = torch.stack(\n",
    "            [torch.tensor(np_arr, dtype=torch.float32)\n",
    "             for np_arr in ground_truth_masks],\n",
    "            dim=0\n",
    "        )\n",
    "        original_sizes = torch.stack(original_sizes, dim=0)\n",
    "        reshaped_input_sizes = torch.stack(reshaped_input_sizes, dim=0)\n",
    "    \n",
    "        pred_masks = []\n",
    "        batch_iou = []\n",
    "        for img_idx in range(len(input_boxes)):\n",
    "            pix_val = pixel_values[img_idx][None, ...]\n",
    "            inp_box = input_boxes[img_idx][None, ...]\n",
    "            \n",
    "            sam_pred = model(\n",
    "                pixel_values=pix_val,\n",
    "                input_boxes=inp_box.to(device),\n",
    "                multimask_output=False\n",
    "            )\n",
    "            \n",
    "            scores = sam_pred.iou_scores\n",
    "    \n",
    "            pred_mask = sam_pred.pred_masks.cpu()\n",
    "            orig_size = original_sizes[img_idx][None, ...].cpu()\n",
    "            reshaped_size = reshaped_input_sizes[img_idx][None, ...].cpu()\n",
    "    \n",
    "            up_pred_mask = processor.image_processor.post_process_masks(\n",
    "                pred_mask,\n",
    "                orig_size,\n",
    "                reshaped_size\n",
    "            )\n",
    "    \n",
    "            # SAM generates a single mask per box, so I'm concatenating them\n",
    "            if inp_box.shape[1] != 1:\n",
    "                up_pred_mask, _ = torch.max(up_pred_mask[0].float(), dim=0)\n",
    "                scores = scores.detach().cpu().numpy().squeeze().mean()\n",
    "    \n",
    "            else:\n",
    "                up_pred_mask = up_pred_mask[0].float()\n",
    "                scores = scores.detach().cpu().numpy().squeeze()\n",
    "                \n",
    "            pred_masks.append(\n",
    "                up_pred_mask.squeeze()\n",
    "            )\n",
    "            batch_iou.append(\n",
    "                scores\n",
    "            )\n",
    "    \n",
    "        # GT\n",
    "        ground_truth_masks = ground_truth_masks.float()\n",
    "        up_pred_masks = torch.stack(pred_masks, dim=0).detach().cpu().numpy().astype(np.uint8)\n",
    "        batch_iou = np.array(batch_iou).mean()\n",
    "        # print(f\"GT {ground_truth_masks.shape} Pred: {up_pred_masks.shape} Scores: {batch_iou}\")\n",
    "    \n",
    "        volume_slices.append(pixel_values.detach().cpu().numpy())\n",
    "        volume_masks.append(ground_truth_masks.detach().cpu().numpy().astype(np.uint8))\n",
    "        volume_pred.append(up_pred_masks.copy())\n",
    "        volume_scores.append(batch_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fdab9c2-3d3e-4a6a-9bc7-80a2307300a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbef1663-8509-4c33-b70d-2d743478fa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 (1, 3, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(len(volume_slices), volume_slices[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6296e6b-97f0-4384-afb7-bea32746d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458, 1282, 929) (385,)\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "\n",
    "zarr_brain_data = zarr.load(\"/scratch/scaled_693196.zarr\")\n",
    "mask_indices = np.array(\n",
    "    natsorted(\n",
    "        [\n",
    "            int(str(p).split(\"_\")[-1].replace('.npy', ''))\n",
    "            for p in Path(\"/scratch/ventricle_dataset/train/images\").glob(\"*.npy\")\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(zarr_brain_data.shape, mask_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f4f9567-e3ca-4f0c-bdf0-f0258d9f97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,\n",
       "        86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "        99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
       "       125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137,\n",
       "       138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
       "       151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163,\n",
       "       164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176,\n",
       "       177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
       "       190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202,\n",
       "       203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "       216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228,\n",
       "       229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
       "       242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254,\n",
       "       255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267,\n",
       "       268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
       "       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
       "       294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306,\n",
       "       307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319,\n",
       "       320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332,\n",
       "       333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345,\n",
       "       346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358,\n",
       "       359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371,\n",
       "       372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384,\n",
       "       385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
       "       398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410,\n",
       "       411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
       "       424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436,\n",
       "       437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449,\n",
       "       450, 451, 452, 453, 454, 455, 456, 457])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67cbd940-e861-46b7-a380-89fe3e76089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "volume_masks_plot = np.concatenate(\n",
    "    volume_masks, axis=0\n",
    ")[:, :, ...]\n",
    "print(volume_masks_plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ced8cd44-10c6-4d6d-a8cf-b01b100b6fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask reshaped:  (385, 1282, 929)\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import zoom\n",
    "\n",
    "scaling_factor = (\n",
    "    1,\n",
    "    zarr_brain_data.shape[-2] / volume_masks_plot.shape[-2],\n",
    "    zarr_brain_data.shape[-1] / volume_masks_plot.shape[-1]\n",
    ")\n",
    "\n",
    "volume_masks_plot = zoom(volume_masks_plot, scaling_factor, order=0)\n",
    "print(\"Mask reshaped: \", volume_masks_plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd341f3d-14af-485d-b25b-5e96830611f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "volume_pred_plot = np.concatenate(\n",
    "    volume_pred, axis=0\n",
    ")[:, :, ...]\n",
    "print(volume_pred_plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f41a58df-023c-4c36-98d4-1615277b712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred reshaped:  (385, 1282, 929)\n"
     ]
    }
   ],
   "source": [
    "volume_pred_plot = zoom(volume_pred_plot, scaling_factor, order=0)\n",
    "print(\"pred reshaped: \", volume_pred_plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc73f819-3c43-4d95-a174-f5b73217217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385, 1282, 929, 3)\n"
     ]
    }
   ],
   "source": [
    "combined_mask = np.zeros(volume_pred_plot.shape + (3, ))\n",
    "print(combined_mask.shape)\n",
    "combined_mask[..., 0] = volume_masks_plot # RED\n",
    "combined_mask[..., 1] = volume_pred_plot # GREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcdb7593-4dd9-4025-b766-533e4bd170b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume_slices_plot = np.expand_dims(\n",
    "#     np.concatenate(\n",
    "#         volume_slices, axis=0\n",
    "#     )[:, 0, :, ...],\n",
    "#     axis=-1\n",
    "# )\n",
    "# # volume_slices_plot = (\n",
    "# #     volume_slices_plot - np.min(volume_slices_plot)\n",
    "# # ) / (np.max(volume_slices_plot) - np.min(volume_slices_plot))\n",
    "\n",
    "# # volume_slices_plot *= 255\n",
    "# print(volume_slices_plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b22197b1-f176-48b7-a6bf-abc26e6c5dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385,)\n"
     ]
    }
   ],
   "source": [
    "volume_scores_plot = np.array(volume_scores)\n",
    "print(volume_scores_plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba21e3ae-f7e9-408f-ac1a-d06f32c30c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04651413da7459cbc056ed5e8ae1e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Slice', max=457), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "# Function to plot a specific slice\n",
    "def plot_slice(volume, combined_mask, scores, mask_indices, slice_idx, cmap='gray'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    slice_ = volume[slice_idx, :, :]\n",
    "\n",
    "    plt.imshow(slice_, cmap=cmap, vmin=0, vmax=5)\n",
    "\n",
    "    msg = f\"Slice {slice_idx}\"\n",
    "    # Plotting mask if it exists in that slice\n",
    "    eval_slice =  np.where(mask_indices == slice_idx)[0]\n",
    "    if eval_slice.shape[0]:\n",
    "        eval_slice = eval_slice[0]\n",
    "    \n",
    "        # slice_msk = mask[eval_slice, :, :]\n",
    "        # slice_pred = mask[eval_slice, :, :]\n",
    "        slice_score = scores[eval_slice]\n",
    "\n",
    "        plt.imshow(combined_mask[eval_slice], alpha=0.5)#, cmap='Greens')\n",
    "        \n",
    "        # plt.imshow(slice_pred, alpha=0.5, cmap='Blues')\n",
    "        # plt.imshow(slice_msk, alpha=0.5, cmap='Oranges')\n",
    "        # plt.scatter(slice_points[:, 1], slice_points[:, 0], c='red', s=5)\n",
    "        msg += f\" - IoU: {slice_score}\"\n",
    "        \n",
    "    plt.title(msg)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Interactive function for controlling slice selection\n",
    "def interactive_plot(volume, combined_mask, scores, mask_indices, axis=0, cmap='gray'):\n",
    "    n_slices = volume.shape[axis]\n",
    "    \n",
    "    # Slider for selecting slices\n",
    "    slice_slider = widgets.IntSlider(min=0, max=n_slices-1, step=1, value=0, description='Slice')\n",
    "    \n",
    "    # Update function for slider\n",
    "    def update(slice_idx):\n",
    "        plot_slice(volume, combined_mask, scores, mask_indices, slice_idx, cmap)\n",
    "\n",
    "    # Interactive display with slider\n",
    "    interact(update, slice_idx=slice_slider)\n",
    "\n",
    "interactive_plot(\n",
    "    zarr_brain_data,\n",
    "    # volume_masks_plot,\n",
    "    # volume_pred_plot,\n",
    "    combined_mask,\n",
    "    volume_scores_plot,\n",
    "    mask_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a938238-0e1d-4c1c-a1c6-8864f2d62f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
